{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/homebrew/lib/python3.10/site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/homebrew/lib/python3.10/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in text data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read one JSON record per line\n",
    "def read_jsonl(f):\n",
    "  f = open(f)\n",
    "  res = []\n",
    "  for line in f:\n",
    "    res.append(json.loads(line))\n",
    "  f.close()\n",
    "  return res\n",
    "\n",
    "data = read_jsonl('dev.json') + read_jsonl('test.json') + read_jsonl('train.json')\n",
    "data = [line['text'] for line in data]\n",
    "doc_count = len(data)\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text)\n",
    "\n",
    "tokens_by_doc = [tokenize(doc) for doc in data[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['under',\n",
       " 'any',\n",
       " 'other',\n",
       " 'circumstances',\n",
       " 'i',\n",
       " 'would',\n",
       " 'not',\n",
       " 'be',\n",
       " 'discussing',\n",
       " 'the',\n",
       " 'ending',\n",
       " 'of',\n",
       " 'a',\n",
       " 'film',\n",
       " 'to',\n",
       " 'the',\n",
       " 'extent',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'in',\n",
       " 'this',\n",
       " 'particular',\n",
       " 'review',\n",
       " 'however',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'fully',\n",
       " 'explain',\n",
       " 'exactly',\n",
       " 'how',\n",
       " 'and',\n",
       " 'why',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'so',\n",
       " 'awful',\n",
       " 'a',\n",
       " 'minute',\n",
       " 'dissection',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ending',\n",
       " 'is',\n",
       " 'necessary',\n",
       " 'even',\n",
       " 'though',\n",
       " 'i',\n",
       " 'will',\n",
       " 'not',\n",
       " 'reveal',\n",
       " 'the',\n",
       " 'details',\n",
       " 'of',\n",
       " 'the',\n",
       " 'last',\n",
       " 'scenes',\n",
       " 'do',\n",
       " 'proceed',\n",
       " 'at',\n",
       " 'your',\n",
       " 'own',\n",
       " 'risk',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'opens',\n",
       " 'quite',\n",
       " 'poorly',\n",
       " 'i',\n",
       " 'might',\n",
       " 'add',\n",
       " 'as',\n",
       " 'child',\n",
       " 'psychologist',\n",
       " 'malcolm',\n",
       " 'crowe',\n",
       " 'bruce',\n",
       " 'willis',\n",
       " 'looking',\n",
       " 'like',\n",
       " 'he',\n",
       " 'was',\n",
       " 'dragged',\n",
       " 'out',\n",
       " 'of',\n",
       " 'his',\n",
       " 'trailer',\n",
       " 'at',\n",
       " 'the',\n",
       " 'wee',\n",
       " 'hours',\n",
       " 'of',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'to',\n",
       " 'shoot',\n",
       " 'each',\n",
       " 'scene',\n",
       " 'and',\n",
       " 'his',\n",
       " 'wife',\n",
       " 'are',\n",
       " 'intruded',\n",
       " 'upon',\n",
       " 'by',\n",
       " 'one',\n",
       " 'of',\n",
       " \"malcolm's\",\n",
       " 'past',\n",
       " 'patients',\n",
       " 'distraught',\n",
       " 'the',\n",
       " 'suicidal',\n",
       " 'man',\n",
       " 'a',\n",
       " 'cameo',\n",
       " 'by',\n",
       " 'new',\n",
       " 'kid',\n",
       " 'on',\n",
       " 'the',\n",
       " 'block',\n",
       " 'donnie',\n",
       " 'wahlberg',\n",
       " 'shoots',\n",
       " 'malcolm',\n",
       " 'and',\n",
       " 'then',\n",
       " 'turns',\n",
       " 'the',\n",
       " 'gun',\n",
       " 'on',\n",
       " 'himself',\n",
       " 'cut',\n",
       " 'to',\n",
       " 'the',\n",
       " 'next',\n",
       " 'fall',\n",
       " 'as',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'good',\n",
       " 'doctor',\n",
       " 'quietly',\n",
       " 'observing',\n",
       " 'his',\n",
       " 'latest',\n",
       " 'case',\n",
       " 'a',\n",
       " 'trouble',\n",
       " 'young',\n",
       " 'man',\n",
       " 'named',\n",
       " 'cole',\n",
       " 'haley',\n",
       " 'joel',\n",
       " 'osment',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'only',\n",
       " 'child',\n",
       " 'actors',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'bludgeoned',\n",
       " 'over',\n",
       " 'the',\n",
       " 'head',\n",
       " 'with',\n",
       " 'a',\n",
       " 'blunt',\n",
       " 'instrument',\n",
       " 'after',\n",
       " 'about',\n",
       " 'minutes',\n",
       " 'of',\n",
       " 'seemingly',\n",
       " 'unrelated',\n",
       " 'freak',\n",
       " 'occurrences',\n",
       " 'we',\n",
       " 'learn',\n",
       " 'that',\n",
       " 'cole',\n",
       " 'has',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'the',\n",
       " 'gift',\n",
       " 'of',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'communicate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'dead',\n",
       " 'and',\n",
       " 'this',\n",
       " 'as',\n",
       " 'they',\n",
       " 'say',\n",
       " 'is',\n",
       " 'where',\n",
       " 'the',\n",
       " 'healing',\n",
       " 'begins',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'and',\n",
       " 'its',\n",
       " 'unexpected',\n",
       " 'popularity',\n",
       " 'is',\n",
       " 'founded',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'twist',\n",
       " 'ending',\n",
       " 'that',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'going',\n",
       " 'into',\n",
       " 'the',\n",
       " 'film',\n",
       " 'one',\n",
       " 'of',\n",
       " 'roger',\n",
       " \"ebert's\",\n",
       " 'colleges',\n",
       " 'was',\n",
       " 'kind',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'on',\n",
       " 'a',\n",
       " 'recent',\n",
       " 'segment',\n",
       " 'of',\n",
       " 'siskel',\n",
       " 'and',\n",
       " 'ebert',\n",
       " 'although',\n",
       " 'i',\n",
       " 'was',\n",
       " 'at',\n",
       " 'first',\n",
       " 'enraged',\n",
       " 'that',\n",
       " 'an',\n",
       " 'established',\n",
       " 'film',\n",
       " 'critic',\n",
       " 'could',\n",
       " 'so',\n",
       " 'callously',\n",
       " 'ruin',\n",
       " 'a',\n",
       " 'film',\n",
       " 'for',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'patrons',\n",
       " 'i',\n",
       " 'soon',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'this',\n",
       " 'turn',\n",
       " 'of',\n",
       " 'events',\n",
       " 'could',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'blessing',\n",
       " 'in',\n",
       " 'disguise',\n",
       " \"i've\",\n",
       " 'always',\n",
       " 'been',\n",
       " 'a',\n",
       " 'sucker',\n",
       " 'for',\n",
       " 'surprise',\n",
       " 'endings',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'suspects',\n",
       " 'and',\n",
       " 'rarely',\n",
       " 'dislike',\n",
       " 'a',\n",
       " 'film',\n",
       " 'that',\n",
       " 'sports',\n",
       " 'one',\n",
       " 'here',\n",
       " 'since',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'the',\n",
       " 'major',\n",
       " 'plot',\n",
       " 'twist',\n",
       " 'that',\n",
       " 'was',\n",
       " 'coming',\n",
       " 'at',\n",
       " 'the',\n",
       " \"film's\",\n",
       " 'conclusion',\n",
       " 'the',\n",
       " 'possibly',\n",
       " 'of',\n",
       " 'being',\n",
       " 'bamboozled',\n",
       " 'into',\n",
       " 'loving',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'movie',\n",
       " 'solely',\n",
       " 'because',\n",
       " 'of',\n",
       " 'its',\n",
       " 'ending',\n",
       " 'something',\n",
       " \"i've\",\n",
       " 'fallen',\n",
       " 'victim',\n",
       " 'to',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " 'was',\n",
       " 'eliminated',\n",
       " 'and',\n",
       " 'indeed',\n",
       " 'my',\n",
       " 'viewing',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'did',\n",
       " 'prove',\n",
       " 'to',\n",
       " 'be',\n",
       " 'quite',\n",
       " 'an',\n",
       " 'enlightening',\n",
       " 'experiment',\n",
       " 'stripped',\n",
       " 'of',\n",
       " 'the',\n",
       " 'element',\n",
       " 'of',\n",
       " 'surprise',\n",
       " 'the',\n",
       " 'film',\n",
       " 'was',\n",
       " 'put',\n",
       " 'to',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'showing',\n",
       " 'what',\n",
       " 'it',\n",
       " 'really',\n",
       " 'had',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'simply',\n",
       " 'hiding',\n",
       " 'behind',\n",
       " 'a',\n",
       " 'shocking',\n",
       " 'conclusion',\n",
       " 'after',\n",
       " 'seeing',\n",
       " 'its',\n",
       " 'true',\n",
       " 'colors',\n",
       " 'i',\n",
       " 'came',\n",
       " 'to',\n",
       " 'the',\n",
       " 'conclusion',\n",
       " 'that',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'is',\n",
       " 'despite',\n",
       " 'what',\n",
       " 'the',\n",
       " 'many',\n",
       " 'champions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'may',\n",
       " 'say',\n",
       " 'void',\n",
       " 'of',\n",
       " 'any',\n",
       " 'real',\n",
       " 'power',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'neat',\n",
       " 'concept',\n",
       " 'but',\n",
       " 'not',\n",
       " 'one',\n",
       " 'that',\n",
       " 'justifies',\n",
       " 'being',\n",
       " 'made',\n",
       " 'into',\n",
       " 'a',\n",
       " 'feature',\n",
       " 'length',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'relies',\n",
       " 'so',\n",
       " 'strongly',\n",
       " 'on',\n",
       " 'its',\n",
       " 'finale',\n",
       " 'that',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'develops',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'prelude',\n",
       " 'to',\n",
       " 'the',\n",
       " 'supposedly',\n",
       " 'earth',\n",
       " 'shattering',\n",
       " 'revelation',\n",
       " 'that',\n",
       " 'is',\n",
       " 'yet',\n",
       " 'to',\n",
       " 'come',\n",
       " 'and',\n",
       " 'when',\n",
       " 'the',\n",
       " 'final',\n",
       " 'moments',\n",
       " 'do',\n",
       " 'come',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'huge',\n",
       " 'letdown',\n",
       " 'the',\n",
       " 'end',\n",
       " 'makes',\n",
       " 'no',\n",
       " 'sense',\n",
       " 'at',\n",
       " 'all',\n",
       " 'it',\n",
       " 'stupefied',\n",
       " 'me',\n",
       " 'with',\n",
       " 'the',\n",
       " 'heights',\n",
       " 'of',\n",
       " 'its',\n",
       " 'ineptitude',\n",
       " 'and',\n",
       " 'is',\n",
       " 'completely',\n",
       " 'idiotic',\n",
       " 'on',\n",
       " 'a',\n",
       " 'fundamental',\n",
       " 'and',\n",
       " 'very',\n",
       " 'rare',\n",
       " 'level',\n",
       " 'i',\n",
       " \"won't\",\n",
       " 'go',\n",
       " 'into',\n",
       " 'any',\n",
       " 'details',\n",
       " 'but',\n",
       " 'suffice',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'as',\n",
       " 'far',\n",
       " 'as',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'it',\n",
       " 'negates',\n",
       " 'to',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'to',\n",
       " 'such',\n",
       " 'an',\n",
       " 'extent',\n",
       " 'that',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'buys',\n",
       " 'it',\n",
       " 'even',\n",
       " 'for',\n",
       " 'a',\n",
       " 'second',\n",
       " 'must',\n",
       " 'be',\n",
       " 'suffering',\n",
       " 'from',\n",
       " 'a',\n",
       " 'very',\n",
       " 'acute',\n",
       " 'case',\n",
       " 'of',\n",
       " 'attention',\n",
       " 'deficit',\n",
       " 'disorder',\n",
       " 'now',\n",
       " 'in',\n",
       " 'all',\n",
       " 'fairness',\n",
       " 'i',\n",
       " 'cannot',\n",
       " 'say',\n",
       " 'for',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'i',\n",
       " 'would',\n",
       " 'have',\n",
       " 'guessed',\n",
       " 'the',\n",
       " 'ending',\n",
       " 'however',\n",
       " 'stupid',\n",
       " 'it',\n",
       " 'may',\n",
       " 'be',\n",
       " 'had',\n",
       " 'it',\n",
       " 'not',\n",
       " 'been',\n",
       " 'revealed',\n",
       " 'to',\n",
       " 'me',\n",
       " 'before',\n",
       " 'hand',\n",
       " 'however',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'confident',\n",
       " 'that',\n",
       " 'i',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'had',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'few',\n",
       " 'twilight',\n",
       " 'zone',\n",
       " 'episodes',\n",
       " 'would',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'it',\n",
       " 'coming',\n",
       " 'a',\n",
       " 'mile',\n",
       " 'away',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'movie',\n",
       " 'goers',\n",
       " 'nation',\n",
       " 'wide',\n",
       " 'are',\n",
       " 'surprised',\n",
       " 'by',\n",
       " 'the',\n",
       " 'ending',\n",
       " 'still',\n",
       " 'has',\n",
       " 'me',\n",
       " 'stumped',\n",
       " 'ironically',\n",
       " 'to',\n",
       " 'fully',\n",
       " 'appreciate',\n",
       " 'the',\n",
       " 'best',\n",
       " 'scene',\n",
       " 'that',\n",
       " 'of',\n",
       " 'cole',\n",
       " 'and',\n",
       " 'malcolm',\n",
       " 'attending',\n",
       " 'a',\n",
       " 'little',\n",
       " \"girl's\",\n",
       " 'funeral',\n",
       " 'the',\n",
       " 'viewer',\n",
       " 'is',\n",
       " 'required',\n",
       " 'to',\n",
       " 'be',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'a',\n",
       " 'very',\n",
       " 'rare',\n",
       " 'psychological',\n",
       " 'disorder',\n",
       " 'called',\n",
       " 'munchausen',\n",
       " 'syndrome',\n",
       " 'by',\n",
       " 'proxy',\n",
       " 'i',\n",
       " \"wouldn't\",\n",
       " 'have',\n",
       " 'even',\n",
       " 'known',\n",
       " 'about',\n",
       " 'this',\n",
       " 'mental',\n",
       " 'disease',\n",
       " 'if',\n",
       " 'i',\n",
       " \"hadn't\",\n",
       " 'by',\n",
       " 'pure',\n",
       " 'dumb',\n",
       " 'luck',\n",
       " 'caught',\n",
       " 'dateline',\n",
       " 'nbc',\n",
       " 'the',\n",
       " 'other',\n",
       " 'week',\n",
       " 'when',\n",
       " 'they',\n",
       " 'did',\n",
       " 'a',\n",
       " 'feature',\n",
       " 'story',\n",
       " 'on',\n",
       " 'it',\n",
       " 'despite',\n",
       " 'being',\n",
       " 'blessed',\n",
       " 'with',\n",
       " 'some',\n",
       " 'really',\n",
       " 'amazing',\n",
       " 'cinematography',\n",
       " 'and',\n",
       " 'a',\n",
       " 'brauva',\n",
       " 'performance',\n",
       " 'from',\n",
       " 'osment',\n",
       " 'where',\n",
       " 'was',\n",
       " 'this',\n",
       " 'kid',\n",
       " 'when',\n",
       " 'casting',\n",
       " 'calls',\n",
       " 'were',\n",
       " 'going',\n",
       " 'out',\n",
       " 'for',\n",
       " 'the',\n",
       " 'phantom',\n",
       " 'menace',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'sense',\n",
       " 'is',\n",
       " 'too',\n",
       " 'chalk',\n",
       " 'full',\n",
       " 'of',\n",
       " 'contradictions',\n",
       " 'and',\n",
       " 'just',\n",
       " \"isn't\",\n",
       " 'plausible',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'warrant',\n",
       " 'even',\n",
       " 'a',\n",
       " 'slight',\n",
       " 'recommendation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_by_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# want to treat the tokens as one long list, rather than a list of tokens per document\n",
    "# to do this, will insert a special character in between each document to stop the training there\n",
    "# TLDR insert a stop token between each document to avoid using next/prev document as context\n",
    "SPECIAL_STOP_TOKEN = '*!!!*'\n",
    "\n",
    "def combine_tokens():\n",
    "  ret = []\n",
    "  for token_list in tokens_by_doc:\n",
    "    ret.extend(token_list)\n",
    "    ret.append(SPECIAL_STOP_TOKEN)\n",
    "\n",
    "  return ret\n",
    "\n",
    "tokens_with_stops = combine_tokens()\n",
    "\n",
    "# should equal number of documents (2000)\n",
    "len([token for token in tokens_with_stops if token == SPECIAL_STOP_TOKEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 'the')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {}\n",
    "id_to_token = {}\n",
    "\n",
    "def make_vocabulary(tokens):\n",
    "  for token in tokens:\n",
    "    if token not in token_to_id:\n",
    "      id_no = len(token_to_id) + 1\n",
    "      token_to_id[token] = id_no\n",
    "      id_to_token[id_no] = token\n",
    "\n",
    "for doc in tokens_by_doc:\n",
    "  make_vocabulary(doc)\n",
    "\n",
    "token_to_id['the'], id_to_token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(token_id):\n",
    "  vector = np.zeros(len(token_to_id) + 1)\n",
    "  vector[token_id] = 1\n",
    "  return vector\n",
    "\n",
    "def yield_range(*ranges):\n",
    "  for iterable in ranges:\n",
    "    yield from iterable\n",
    "\n",
    "def create_dataset(window_size=2):\n",
    "  X = []\n",
    "  y = []\n",
    "  \n",
    "  # note that this includes stop tokens, which will need to be skipped\n",
    "  token_count = len(tokens_with_stops)\n",
    "\n",
    "  for i in range(token_count):\n",
    "    if tokens_with_stops[i] == SPECIAL_STOP_TOKEN:\n",
    "      continue\n",
    "\n",
    "    indexes = yield_range(range(max(0, i - window_size), i), range(i, min(token_count, i + window_size + 1)))\n",
    "    for j in indexes:\n",
    "      if j == i:\n",
    "        continue\n",
    "      \n",
    "      if tokens_with_stops[j] == SPECIAL_STOP_TOKEN:\n",
    "        break\n",
    "\n",
    "      X.append(create_one_hot(token_to_id[tokens_with_stops[i]]))\n",
    "      y.append(create_one_hot(token_to_id[tokens_with_stops[j]]))\n",
    "\n",
    "  return np.asarray(X), np.asarray(y)\n",
    "\n",
    "X, y = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3022, 383), (3022, 383))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Initialize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size: int, n_embeddings: int) -> dict:\n",
    "  model = {\n",
    "    'w1': np.random.randn(vocab_size + 1, n_embeddings),\n",
    "    'w2': np.random.randn(n_embeddings, vocab_size + 1),\n",
    "  }\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(outputs):\n",
    "  res = []\n",
    "  for x in outputs:\n",
    "    res.append(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "  return res\n",
    "\n",
    "def safe_log(array):\n",
    "  for i in range(len(array)):\n",
    "    for x in range(len(array[i])):\n",
    "      if array[i][x] > 0:\n",
    "        array[i][x] = np.log(array[i][x])\n",
    "      # else:\n",
    "      #   array[i][x] = 0\n",
    "  return array\n",
    "\n",
    "def cross_entropy_loss(z, y):\n",
    "  # print(y)\n",
    "  # print(z)\n",
    "  # return - np.sum(y * np.log(z))\n",
    "  print(- np.sum(y * safe_log(z)))\n",
    "  return - np.sum(y * safe_log(z))\n",
    "\n",
    "def forward_prop(model, X):\n",
    "  cache = {}\n",
    "  cache['a1'] = X @ model['w1']\n",
    "  cache['a2'] = cache['a1'] @ model['w2']\n",
    "  cache['z'] = softmax(cache['a2'])\n",
    "  return cache\n",
    "\n",
    "def back_prop(model, X, y, alpha):\n",
    "  cache = forward_prop(model, X)\n",
    "  da2 = cache['z'] - y\n",
    "  dw2 = cache['a1'].T @ da2\n",
    "  da1 = da2 @ model['w2'].T\n",
    "  dw1 = X.T @ da1\n",
    "  model['w1'] -= alpha * dw1\n",
    "  model['w2'] -= alpha * dw2\n",
    "  return cross_entropy_loss(cache['z'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "sns.set()\n",
    "\n",
    "n_iter = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "model = init_network(len(token_to_id), 10)\n",
    "model['w1'].shape, model['w2'].shape\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "  back_prop(model, X, y, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "  try:\n",
    "    idx = token_to_id[word]\n",
    "  except KeyError:\n",
    "    print(f'{word} not in corpus')\n",
    "  one_hot = create_one_hot(idx)\n",
    "  return forward_prop(model, one_hot)['a1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40688602e+55, -1.10557696e+55,  4.75933117e+55,  1.59856214e+55,\n",
       "        8.85353780e+54, -1.32013601e+55, -1.24125605e+55,  3.68753596e+55,\n",
       "        2.66791727e+55,  2.13621542e+55])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(model, 'film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30258.83980074257\n",
      "25499.703353135912\n",
      "23187.03572040419\n",
      "21826.54677761536\n",
      "20827.17026249612\n",
      "20046.502360861326\n",
      "19431.34072163342\n",
      "18906.248128160365\n",
      "18455.416717601485\n",
      "18059.13914339643\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "n_iter = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = init_network(len(token_to_id), 10)\n",
    "history = [back_prop(model, X, y, learning_rate) for _ in range(n_iter)]\n",
    "\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
