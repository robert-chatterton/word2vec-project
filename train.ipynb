{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/robbie/.local/lib/python3.8/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in text data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read one JSON record per line\n",
    "def read_jsonl(f):\n",
    "  f = open(f)\n",
    "  res = []\n",
    "  for line in f:\n",
    "    res.append(json.loads(line))\n",
    "  f.close()\n",
    "  return res\n",
    "\n",
    "data = read_jsonl('dev.json') + read_jsonl('test.json') + read_jsonl('train.json')\n",
    "data = [line['text'] for line in data]\n",
    "doc_count = len(data)\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text)\n",
    "\n",
    "tokens_by_doc = [tokenize(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# want to treat the tokens as one long list, rather than a list of tokens per document\n",
    "# to do this, will insert a special character in between each document to stop the training there\n",
    "# TLDR insert a stop token between each document to avoid using next/prev document as context\n",
    "SPECIAL_STOP_TOKEN = '*!!!*'\n",
    "\n",
    "def combine_tokens():\n",
    "  ret = []\n",
    "  for token_list in tokens_by_doc:\n",
    "    ret.extend(token_list)\n",
    "    ret.append(SPECIAL_STOP_TOKEN)\n",
    "\n",
    "  return ret\n",
    "\n",
    "tokens_with_stops = combine_tokens()\n",
    "\n",
    "# should equal number of documents (2000)\n",
    "len([token for token in tokens_with_stops if token == SPECIAL_STOP_TOKEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 'the')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {}\n",
    "id_to_token = {}\n",
    "\n",
    "def make_vocabulary(tokens):\n",
    "  for token in tokens:\n",
    "    if token not in token_to_id:\n",
    "      id_no = len(token_to_id) + 1\n",
    "      token_to_id[token] = id_no\n",
    "      id_to_token[id_no] = token\n",
    "\n",
    "for doc in tokens_by_doc:\n",
    "  make_vocabulary(doc)\n",
    "\n",
    "token_to_id['the'], id_to_token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_one_hot(token_id):\n",
    "  vector = np.zeros(len(token_to_id))\n",
    "  vector[token_id] = 1\n",
    "  return vector\n",
    "\n",
    "def yield_range(*ranges):\n",
    "  for iterable in ranges:\n",
    "    yield from iterable\n",
    "\n",
    "def create_dataset(window_size=2):\n",
    "  X = []\n",
    "  y = []\n",
    "  \n",
    "  # note that this includes stop tokens, which will need to be skipped\n",
    "  token_count = len(tokens_with_stops)\n",
    "\n",
    "  for i in range(token_count):\n",
    "    if tokens_with_stops[i] == SPECIAL_STOP_TOKEN:\n",
    "      continue\n",
    "\n",
    "    indexes = yield_range(range(max(0, i - window_size), i), range(i, min(token_count, i + window_size + 1)))\n",
    "    for j in indexes:\n",
    "      if j == i:\n",
    "        continue\n",
    "      \n",
    "      if tokens_with_stops[j] == SPECIAL_STOP_TOKEN:\n",
    "        break\n",
    "\n",
    "      X.append(create_one_hot(token_to_id[tokens_with_stops[i]]))\n",
    "      y.append(create_one_hot(token_to_id[tokens_with_stops[j]]))\n",
    "\n",
    "  return X, y\n",
    "\n",
    "X, y = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_X \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_X \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39;49m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mdataset_X.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, all_X)\n\u001b[1;32m      2\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mdataset_y.npy\u001b[39m\u001b[39m'\u001b[39m, all_y)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:522\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_array(fid, arr, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    523\u001b[0m                        pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(fix_imports\u001b[39m=\u001b[39;49mfix_imports))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/format.py:700\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[39mif\u001b[39;00m pickle_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m         pickle_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 700\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(array, fp, protocol\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    701\u001b[0m \u001b[39melif\u001b[39;00m array\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m array\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous:\n\u001b[1;32m    702\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "np.save('dataset_X.npy', all_X)\n",
    "np.save('dataset_y.npy', all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
