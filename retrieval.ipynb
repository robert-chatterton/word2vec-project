{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserini==0.12.0 in /opt/homebrew/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (4.64.1)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (4.25.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (0.1.97)\n",
      "Requirement already satisfied: pyjnius>=1.2.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.4.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.5.2)\n",
      "Requirement already satisfied: Cython>=0.29.21 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (0.29.32)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.1.5->pyserini==0.12.0) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from pandas>=1.1.5->pyserini==0.12.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from pyjnius>=1.2.1->pyserini==0.12.0) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=0.22.1->pyserini==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=0.22.1->pyserini==0.12.0) (3.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (2.28.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (3.8.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.0.0->pyserini==0.12.0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from packaging>=20.0->transformers>=4.0.0->pyserini==0.12.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement glove-python-binary (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for glove-python-binary\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting glove\n",
      "  Using cached glove-1.0.2.tar.gz (44 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from glove) (1.23.5)\n",
      "Building wheels for collected packages: glove\n",
      "  Building wheel for glove (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/__init__.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/glove.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing glove.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to glove.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to glove.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: Error: setup script specifies an absolute path:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     /private/var/folders/pf/yhbc_83j0_16nz__1n8lp9qr0000gn/T/pip-install-95oqrdtq/glove_24f3d3528734495baf20eaf27ad9dc38/glove/glove_inner.pyx\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m setup() arguments must *always* be /-separated paths relative to the\n",
      "  \u001b[31m   \u001b[0m setup.py directory, *never* absolute paths.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for glove\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for glove\n",
      "Failed to build glove\n",
      "Installing collected packages: glove\n",
      "  Running setup.py install for glove ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for glove\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[25 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /opt/homebrew/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/__init__.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/glove.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing glove.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to glove.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to glove.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: Error: setup script specifies an absolute path:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     /private/var/folders/pf/yhbc_83j0_16nz__1n8lp9qr0000gn/T/pip-install-95oqrdtq/glove_24f3d3528734495baf20eaf27ad9dc38/glove/glove_inner.pyx\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m setup() arguments must *always* be /-separated paths relative to the\n",
      "  \u001b[31m   \u001b[0m setup.py directory, *never* absolute paths.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m glove\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install pyserini==0.12.0\n",
    "%pip install glove-python-binary\n",
    "%pip install glove\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "from pyserini import search\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174787 FT923-3828\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "qfile = 'https://github.com/castorini/anserini-tools/blob/63ceeab1dd94c1221f29b931d868e8fab67cc25c/topics-and-qrels/qrels.robust04.txt?raw=true'\n",
    "docids = set()\n",
    "# queryId -> docId -> relevanceScore\n",
    "qrels = dict()\n",
    "for line in urlopen(qfile):\n",
    "  qid, round, docid, score = line.strip().split()\n",
    "  new_docid = docid.decode('UTF-8')\n",
    "  docids.add(new_docid)\n",
    "\n",
    "  if int(qid) in qrels:\n",
    "    qrels[int(qid)][new_docid] = int(score)\n",
    "  else:\n",
    "    qrels[int(qid)] = dict()\n",
    "    qrels[int(qid)][new_docid] = int(score)\n",
    "\n",
    "\n",
    "docids = list(docids)\n",
    "np.random.shuffle(docids)\n",
    "print(len(docids), docids[0])\n",
    "print(len(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index robust04.\n",
      "/Users/hstack/.cache/pyserini/indexes/index-robust04-20191213.15f3d001489c97849a010b0a4734d018 already exists, skipping download.\n",
      "Initializing robust04...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('FT923-3828',\n",
       " ' 920910     FT  10 SEP 92 / Technology: Lifes little ups and downs - Escalators may have a humdrum role but building the longest in western Europe has proved to be no easy task     The diary of Richard Burbage, manager of Harrods in 1898, recalls that shoppers overcome by the experience of travelling on Britains first escalator were revived with brandy or sal volatile dispensed by an attendant positioned at the top. Burbage, according to the Shell Book of Firsts, had an unreasoning dislike of lifts, and persuaded the Knightsbridge store to instal the 40ft-long Reno Inclined Elevator, patented by Jesse W. Reno of New York just six years earlier. Nearly a century later, would-be passengers on the longest escalators in western Europe, at the new Angel station on the London Underground, might feel they deserve similar resuscitation after the problems of the past month. Dogged by difficulties, the three 60m-long escalators are still not functioning properly, delaying the opening of the long-awaited Pounds 70m new station for Islingtons commuters. But an end to their long wait is in sight. The delay has been irritating for passengers, who have been shuttled around by bus while the new station stays closed, and embarrassing for both the London Underground and Construction Industrielle de la Mediterrannee (better known by its logo CNIM), the big Marseilles-based company which designed and built the escalator. But the saga has also been embarrassing for the escalator industry, which is not used to having the reliability of its products questioned. Escalators, after all, are relatively simple people movers which are built to last, and normally do. Some of the London Undergrounds escalators are now being replaced after as much as 60 years of sterling service. These escalators are very heavy duty. They take a hell of a beating, says one producer. Switched on in the morning, and off late at night - or occasionally left on permanently - escalators simply keep on going, requiring routine maintenance but rarely any fundamental rebuilding during their working lives. The basic technology is tried and tested. The first step-type escalator with a comb-plate landing device was introduced commercially in 1921 by Otis, the US lift and escalator producer which is now part of United Technologies, and the basic combination of an electric motor driving a chain held by two sprockets has not changed since then. There have, naturally, been developments in escalator safety circuits, such as systems to stop the motor if a passengers hand goes too far down the handrail at either end, and in the use of materials. Again safety has been the prime consideration, with wooden escalators now seen as a fire risk following the Kings Cross disaster in 1987. There have been innovations such as energy-saving stop-start mechanisms  - approach an apparently resting escalator on the Munich subway and it will start rolling just before you step on it - and gimmicks such as helical escalators, which are both technically daunting and commercially dependent on booming markets for swanky offices and department stores. In general, though, escalators, because of their humdrum, continuous task, have missed out on the major developments in lift technology introduced by the same big producers over the past 20 years - variable speed drives, talking lifts, lifts with memories and remote monitoring techniques. This is what makes the Angel saga so frustrating for commuters. Things happen in lifts, because the relationship between the passenger and machine is much more complex, but escalators are simply not expected to break down. Angel Station first started having capacity problems in the mid-1980s, when Islington became an increasingly popular place to live, and passenger volume rose about six-fold when volumes elsewhere were shrinking. At that time the station had no escalators, only three lifts, each with capacity for about 30 people, and which were slow and unreliable. The station serves 8m people a year (about 20,000 a day) so the lifts were clearly inadequate. Reconstruction of the station started in 1989, and preliminary work on construction of the escalators started more than a year ago. Everything went according to plan until the last few days before the escalators were due to come into service, on Sunday August 9. Safety regulations require a commissioning test, when the escalators must run for 24 hours continuously with no problems. This was carried out on Friday 7 and Saturday 8 August, when the first problems emerged with the power supply. The supply in the area was not sufficient to serve the whole station and the escalators at the same time. While adjustments were carried out, the opening was delayed until Wednesday August 12. The station was then open for a day and a half, after which further problems emerged with the control electronics, partly related to the earlier power supply problems, causing the escalators to cut out when fully loaded. A faulty chain brake switch was erroneously signalling that the chain was sagging as soon as passengers got on the escalator, thus bringing it to a stop. The glitch was corrected, and a final extensive test carried out on August 18, when some 400 LU employees travelled up and down the escalator to ensure it was running smoothly. This was crucial for safety reasons, as the sudden stopping of a fully-laden escalator - about 180 steps with two people on each one - could be potentially disastrous by causing a human avalanche. The escalator passed that test, but in the process engineers discovered gaps between the steps and the skirtings of as much as 9mm in total on both sides, large enough for a childs finger to become caught, or for small objects to slip down into the machinery. The opening was delayed once again. London Underground and CNIM will not go into detail about the causes of the problem, but industry observers suggest some possibilities. First, the Underground traditionally specifies a fabricated step, which is welded together from pieces of steel and then has an aluminium alloy tread bolted on top. It says this lasts much longer than the one-piece die-cast step used on other metro systems. There is some debate about how much more durable the fabricated step is, but at about 45kg it is three times heavier than a die-cast step and much harder to manufacture to very tight tolerances because of the welding and subsequent cooling which distort the steel. Die-cast steps, in contrast, can be milled to very high levels of precision, says one escalator producer. And LU requires the gap between the steps and the skirting to be no more than 5mm in total over both sides - tougher than the 7mm British Standard recommendation. With wear and tear, the gap will widen, so the tighter specification gives LU some leeway. The features of a fabricated step become particularly important on a long escalator, which is like a bicycle chain on two sprockets. The weight of so many fabricated steps may have contributed to the earlier power problems, and the longer the chain, the more opportunity there is for lateral movement. That could have increased the gaps between the treads and the skirtings. Historically, escalators have simply been side-guided - the skirting keeps the train in line on the track - but a number of guidance systems are now used, such as a lipped edge to the track, or a ridge into which the chain interlocks. CNIM says that the guidance system it is using at the Angel is the same as it uses on all its escalators, long or short. The French company has been given three weeks to sort out the gaps, and the deadline elapses next Thursday. But yesterday London Underground said it was hopeful of being able to run a further 24-hour test to ensure that the problem has been resolved, and open the escalators to the public, before then. The gap is now down to about 6mm, spread over both sides, and the target is 4mm. Islingtons long-suffering commuters will then have their new station, but that might not be the end of the affair. LU will reportedly claim hundreds of thousands of pounds compensation from CNIM, but says officially that seeking compensation will depend on the final length of the delay. Its managers, meanwhile, might be forgiven for resorting to the Burbage remedy for escalator angst.  ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "searcher = search.SimpleSearcher.from_prebuilt_index('robust04')\n",
    "for docid in docids[:50]:\n",
    "  try:\n",
    "    text = searcher.doc(docid).raw()\n",
    "    data.append((docid, text))\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "cleaned_data = []\n",
    "for docid, text in data:\n",
    "  new_text = text.replace('<P>', ' ')\n",
    "  new_text = new_text.replace('</P>', ' ')\n",
    "  new_text = new_text.replace('<DATE>', ' ')\n",
    "  new_text = new_text.replace('</DATE>', ' ')\n",
    "  new_text = new_text.replace('<HEADLINE>', ' ')\n",
    "  new_text = new_text.replace('</HEADLINE>', ' ')\n",
    "  new_text = new_text.replace('<TEXT>', ' ')\n",
    "  new_text = new_text.replace('</TEXT>', ' ')\n",
    "  new_text = new_text.replace('\\n', ' ')\n",
    "  new_text = new_text.replace(\"\\'\", '')\n",
    "  new_text = new_text.replace('--', ' ')\n",
    "  cleaned_data.append((docid, new_text))\n",
    "  \n",
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FT923-3828', ['ft', 'sep', 'technolog', 'life', 'littl', 'escal', 'may', 'humdrum', 'role', 'build', 'longest', 'western', 'europ', 'ha', 'prove', 'easi', 'task', 'diari', 'richard', 'burbag', 'manag', 'harrod', 'recal', 'shopper', 'overcom', 'experi', 'travel', 'britain', 'first', 'escal', 'reviv', 'brandi', 'sal', 'volatil', 'dispens', 'attend', 'posit', 'top', 'burbag', 'accord', 'shell', 'book', 'first', 'unreason', 'dislik', 'lift', 'persuad', 'knightsbridg', 'store', 'instal', '40ft', 'long', 'reno', 'inclin', 'elev', 'patent', 'jess', 'w', 'reno', 'new', 'york', 'six', 'year', 'earlier', 'nearli', 'centuri', 'later', 'would', 'passeng', 'longest', 'escal', 'western', 'europ', 'new', 'angel', 'station', 'london', 'underground', 'might', 'feel', 'deserv', 'similar', 'resuscit', 'problem', 'past', 'month', 'dog', 'difficulti', 'three', '60m', 'long', 'escal', 'still', 'function', 'properli', 'delay', 'open', 'long', 'await', 'pound', '70m', 'new', 'station', 'islington', 'commut', 'end', 'long', 'wait', 'sight', 'delay', 'ha', 'irrit', 'passeng', 'shuttl', 'around', 'bu', 'new', 'station', 'stay', 'close', 'embarrass', 'london', 'underground', 'construct', 'industriel', 'de', 'la', 'mediterranne', 'better', 'known', 'logo', 'cnim', 'big', 'marseil', 'base', 'compani', 'design', 'built', 'escal', 'saga', 'ha', 'also', 'embarrass', 'escal', 'industri', 'use', 'reliabl', 'product', 'question', 'escal', 'rel', 'simpl', 'peopl', 'mover', 'built', 'last', 'normal', 'london', 'underground', 'escal', 'replac', 'much', 'year', 'sterl', 'servic', 'escal', 'veri', 'heavi', 'duti', 'take', 'hell', 'beat', 'say', 'one', 'produc', 'switch', 'morn', 'late', 'night', 'occasion', 'left', 'perman', 'escal', 'simpli', 'keep', 'go', 'requir', 'routin', 'mainten', 'rare', 'ani', 'fundament', 'rebuild', 'dure', 'work', 'live', 'basic', 'technolog', 'tri', 'test', 'first', 'step', 'type', 'escal', 'comb', 'plate', 'land', 'devic', 'wa', 'introduc', 'commerci', 'oti', 'us', 'lift', 'escal', 'produc', 'part', 'unit', 'technolog', 'basic', 'combin', 'electr', 'motor', 'drive', 'chain', 'held', 'two', 'sprocket', 'ha', 'chang', 'sinc', 'natur', 'develop', 'escal', 'safeti', 'circuit', 'system', 'stop', 'motor', 'passeng', 'hand', 'goe', 'far', 'handrail', 'either', 'end', 'use', 'materi', 'safeti', 'ha', 'prime', 'consider', 'wooden', 'escal', 'seen', 'fire', 'risk', 'follow', 'king', 'cross', 'disast', 'innov', 'energi', 'save', 'stop', 'start', 'mechan', 'approach', 'appar', 'rest', 'escal', 'munich', 'subway', 'start', 'roll', 'befor', 'step', 'gimmick', 'helic', 'escal', 'technic', 'daunt', 'commerci', 'depend', 'boom', 'market', 'swanki', 'offic', 'depart', 'store', 'gener', 'though', 'escal', 'becaus', 'humdrum', 'continu', 'task', 'miss', 'major', 'develop', 'lift', 'technolog', 'introduc', 'big', 'produc', 'past', 'year', 'variabl', 'speed', 'drive', 'talk', 'lift', 'lift', 'memori', 'remot', 'monitor', 'techniqu', 'thi', 'make', 'angel', 'saga', 'frustrat', 'commut', 'thing', 'happen', 'lift', 'becaus', 'relationship', 'passeng', 'machin', 'much', 'complex', 'escal', 'simpli', 'expect', 'break', 'angel', 'station', 'first', 'start', 'capac', 'problem', 'mid', '1980', 'islington', 'becam', 'increasingli', 'popular', 'place', 'live', 'passeng', 'volum', 'rose', 'six', 'fold', 'volum', 'elsewher', 'shrink', 'time', 'station', 'escal', 'onli', 'three', 'lift', 'capac', 'peopl', 'slow', 'unreli', 'station', 'serv', '8m', 'peopl', 'year', 'day', 'lift', 'clearli', 'inadequ', 'reconstruct', 'station', 'start', 'preliminari', 'work', 'construct', 'escal', 'start', 'year', 'ago', 'everyth', 'went', 'accord', 'plan', 'last', 'day', 'befor', 'escal', 'due', 'come', 'servic', 'sunday', 'august', 'safeti', 'regul', 'requir', 'commiss', 'test', 'escal', 'must', 'run', 'hour', 'continu', 'problem', 'thi', 'wa', 'carri', 'friday', 'saturday', 'august', 'first', 'problem', 'emerg', 'power', 'suppli', 'suppli', 'area', 'wa', 'suffici', 'serv', 'whole', 'station', 'escal', 'time', 'adjust', 'carri', 'open', 'wa', 'delay', 'wednesday', 'august', 'station', 'wa', 'open', 'day', 'half', 'problem', 'emerg', 'control', 'electron', 'partli', 'relat', 'earlier', 'power', 'suppli', 'problem', 'caus', 'escal', 'cut', 'fulli', 'load', 'faulti', 'chain', 'brake', 'switch', 'wa', 'erron', 'signal', 'chain', 'wa', 'sag', 'soon', 'passeng', 'got', 'escal', 'thu', 'bring', 'stop', 'glitch', 'wa', 'correct', 'final', 'extens', 'test', 'carri', 'august', 'lu', 'employe', 'travel', 'escal', 'ensur', 'wa', 'run', 'smoothli', 'thi', 'wa', 'crucial', 'safeti', 'reason', 'sudden', 'stop', 'fulli', 'laden', 'escal', 'step', 'two', 'peopl', 'one', 'could', 'potenti', 'disastr', 'caus', 'human', 'avalanch', 'escal', 'pass', 'test', 'process', 'engin', 'discov', 'gap', 'step', 'skirt', 'much', '9mm', 'total', 'side', 'larg', 'enough', 'child', 'finger', 'becom', 'caught', 'small', 'object', 'slip', 'machineri', 'open', 'wa', 'delay', 'onc', 'london', 'underground', 'cnim', 'go', 'detail', 'caus', 'problem', 'industri', 'observ', 'suggest', 'possibl', 'first', 'underground', 'tradit', 'specifi', 'fabric', 'step', 'weld', 'togeth', 'piec', 'steel', 'ha', 'aluminium', 'alloy', 'tread', 'bolt', 'top', 'say', 'thi', 'last', 'much', 'longer', 'one', 'piec', 'die', 'cast', 'step', 'use', 'metro', 'system', 'debat', 'much', 'durabl', 'fabric', 'step', '45kg', 'three', 'time', 'heavier', 'die', 'cast', 'step', 'much', 'harder', 'manufactur', 'veri', 'tight', 'toler', 'becaus', 'weld', 'subsequ', 'cool', 'distort', 'steel', 'die', 'cast', 'step', 'contrast', 'mill', 'veri', 'high', 'level', 'precis', 'say', 'one', 'escal', 'produc', 'lu', 'requir', 'gap', 'step', 'skirt', '5mm', 'total', 'side', 'tougher', '7mm', 'british', 'standard', 'recommend', 'wear', 'tear', 'gap', 'widen', 'tighter', 'specif', 'give', 'lu', 'leeway', 'featur', 'fabric', 'step', 'becom', 'particularli', 'import', 'long', 'escal', 'like', 'bicycl', 'chain', 'two', 'sprocket', 'weight', 'mani', 'fabric', 'step', 'may', 'contribut', 'earlier', 'power', 'problem', 'longer', 'chain', 'opportun', 'later', 'movement', 'could', 'increas', 'gap', 'tread', 'skirt', 'histor', 'escal', 'simpli', 'side', 'guid', 'skirt', 'keep', 'train', 'line', 'track', 'number', 'guidanc', 'system', 'use', 'lip', 'edg', 'track', 'ridg', 'chain', 'interlock', 'cnim', 'say', 'guidanc', 'system', 'use', 'angel', 'use', 'escal', 'long', 'short', 'french', 'compani', 'ha', 'given', 'three', 'week', 'sort', 'gap', 'deadlin', 'elaps', 'next', 'thursday', 'yesterday', 'london', 'underground', 'said', 'wa', 'hope', 'abl', 'run', 'hour', 'test', 'ensur', 'problem', 'ha', 'resolv', 'open', 'escal', 'public', 'befor', 'gap', '6mm', 'spread', 'side', 'target', '4mm', 'islington', 'long', 'suffer', 'commut', 'new', 'station', 'might', 'end', 'affair', 'lu', 'reportedli', 'claim', 'hundr', 'thousand', 'pound', 'compens', 'cnim', 'say', 'offici', 'seek', 'compens', 'depend', 'final', 'length', 'delay', 'manag', 'meanwhil', 'might', 'forgiven', 'resort', 'burbag', 'remedi', 'escal', 'angst'])\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def tokenize(doc_id_and_text):\n",
    "    text = doc_id_and_text[1]\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    tokens = pattern.findall(text)\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "      stemmed_token = stemmer.stem(token.replace(\"'\", ''))\n",
    "      if stemmed_token not in stop_words:\n",
    "        stemmed_tokens.append(stemmed_token)\n",
    "    return (doc_id_and_text[0],stemmed_tokens)\n",
    "\n",
    "tokens_by_doc = [tokenize(doc) for doc in cleaned_data[:25]]\n",
    "print(tokens_by_doc[0])\n",
    "\n",
    "doc_id_to_tokens = dict()\n",
    "for tokens in tokens_by_doc:\n",
    "  doc_id_to_tokens[tokens[0]] = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "350\n",
      "Health and Computer Terminals\n",
      "351\n",
      "Falkland petroleum exploration\n",
      "352\n",
      "British Chunnel impact\n",
      "353\n",
      "Antarctica exploration\n",
      "354\n",
      "journalist risks\n",
      "355\n",
      "ocean remote sensing\n",
      "356\n",
      "postmenopausal estrogen Britain\n",
      "357\n",
      "territorial waters dispute\n",
      "358\n",
      "blood-alcohol fatalities\n",
      "359\n",
      "mutual fund predictors\n",
      "360\n",
      "drug legalization benefits\n",
      "361\n",
      "clothing sweatshops\n",
      "362\n",
      "human smuggling\n",
      "363\n",
      "transportation tunnel disasters\n",
      "364\n",
      "rabies\n",
      "365\n",
      "El Nino\n",
      "366\n",
      "commercial cyanide uses\n",
      "367\n",
      "piracy\n",
      "368\n",
      "in vitro fertilization\n",
      "369\n",
      "anorexia nervosa bulimia\n",
      "370\n",
      "food/drug laws\n",
      "371\n",
      "health insurance holistic\n",
      "372\n",
      "Native American casino\n",
      "373\n",
      "encryption equipment export\n",
      "374\n",
      "Nobel prize winners\n",
      "375\n",
      "hydrogen energy\n",
      "376\n",
      "World Court\n",
      "377\n",
      "cigar smoking\n",
      "378\n",
      "euro opposition\n",
      "379\n",
      "mainstreaming\n",
      "380\n",
      "obesity medical treatment\n",
      "381\n",
      "alternative medicine\n",
      "382\n",
      "hydrogen fuel automobiles\n",
      "383\n",
      "mental illness drugs\n",
      "384\n",
      "space station moon\n",
      "385\n",
      "hybrid fuel cars\n",
      "386\n",
      "teaching disabled children\n",
      "387\n",
      "radioactive waste\n",
      "388\n",
      "organic soil enhancement\n",
      "389\n",
      "illegal technology transfer\n",
      "700\n",
      "gasoline tax U.S.\n",
      "390\n",
      "orphan drugs\n",
      "391\n",
      "R&D drug prices\n",
      "392\n",
      "robotics\n",
      "393\n",
      "mercy killing\n",
      "394\n",
      "home schooling\n",
      "395\n",
      "tourism\n",
      "396\n",
      "sick building syndrome\n",
      "397\n",
      "automobile recalls\n",
      "398\n",
      "dismantling Europe's arsenal\n",
      "399\n",
      "oceanographic vessels\n",
      "601\n",
      "Turkey Iraq water\n",
      "602\n",
      "Czech, Slovak sovereignty\n",
      "603\n",
      "Tobacco cigarette lawsuit\n",
      "604\n",
      "Lyme disease arthritis\n",
      "605\n",
      "Great Britain health care\n",
      "606\n",
      "leg traps ban\n",
      "607\n",
      "human genetic code\n",
      "608\n",
      "taxing social security\n",
      "609\n",
      "per capita alcohol consumption\n",
      "610\n",
      "minimum wage adverse impact\n",
      "611\n",
      "Kurds Germany violence\n",
      "612\n",
      "Tibet protesters\n",
      "613\n",
      "Berlin wall disposal\n",
      "614\n",
      "Flavr Savr tomato\n",
      "615\n",
      "timber exports Asia\n",
      "616\n",
      "Volkswagen Mexico\n",
      "617\n",
      "Russia Cuba economy\n",
      "618\n",
      "Ayatollah Khomeini death\n",
      "619\n",
      "Winnie Mandela scandal\n",
      "620\n",
      "France nuclear testing\n",
      "621\n",
      "women ordained Church of England\n",
      "622\n",
      "price fixing\n",
      "623\n",
      "toxic chemical weapon\n",
      "624\n",
      "SDI Star Wars\n",
      "625\n",
      "arrests bombing WTC\n",
      "626\n",
      "human stampede\n",
      "627\n",
      "Russian food crisis\n",
      "628\n",
      "U.S. invasion of Panama\n",
      "629\n",
      "abortion clinic attack\n",
      "630\n",
      "Gulf War Syndrome\n",
      "631\n",
      "Mandela South Africa President\n",
      "632\n",
      "southeast Asia tin mining\n",
      "633\n",
      "Welsh devolution\n",
      "634\n",
      "L-tryptophan deaths\n",
      "635\n",
      "doctor assisted suicides\n",
      "636\n",
      "jury duty exemptions\n",
      "637\n",
      "human growth hormone (HGH)\n",
      "638\n",
      "wrongful convictions\n",
      "639\n",
      "consumer on-line shopping\n",
      "640\n",
      "maternity leave policies\n",
      "641\n",
      "Valdez wildlife marine life\n",
      "400\n",
      "Amazon rain forest\n",
      "642\n",
      "Tiananmen Square protesters\n",
      "401\n",
      "foreign minorities, Germany\n",
      "643\n",
      "salmon dams Pacific northwest\n",
      "402\n",
      "behavioral genetics\n",
      "644\n",
      "exotic animals import\n",
      "403\n",
      "osteoporosis\n",
      "645\n",
      "software piracy\n",
      "404\n",
      "Ireland, peace talks\n",
      "646\n",
      "food stamps increase\n",
      "405\n",
      "cosmic events\n",
      "647\n",
      "windmill electricity\n",
      "406\n",
      "Parkinson's disease\n",
      "648\n",
      "family leave law\n",
      "407\n",
      "poaching, wildlife preserves\n",
      "649\n",
      "computer viruses\n",
      "408\n",
      "tropical storms\n",
      "409\n",
      "legal, Pan Am, 103\n",
      "650\n",
      "tax evasion indicted\n",
      "651\n",
      "U.S. ethnic population\n",
      "410\n",
      "Schengen agreement\n",
      "652\n",
      "OIC Balkans 1990s\n",
      "411\n",
      "salvaging, shipwreck, treasure\n",
      "653\n",
      "ETA Basque terrorism\n",
      "412\n",
      "airport security\n",
      "654\n",
      "same-sex schools\n",
      "413\n",
      "steel production\n",
      "655\n",
      "ADD diagnosis treatment\n",
      "414\n",
      "Cuba, sugar, exports\n",
      "656\n",
      "lead poisoning children\n",
      "415\n",
      "drugs, Golden Triangle\n",
      "657\n",
      "school prayer banned\n",
      "416\n",
      "Three Gorges Project\n",
      "658\n",
      "teenage pregnancy\n",
      "417\n",
      "creativity\n",
      "659\n",
      "cruise health safety\n",
      "418\n",
      "quilts, income\n",
      "419\n",
      "recycle, automobile tires\n",
      "660\n",
      "whale watching California\n",
      "661\n",
      "melanoma treatment causes\n",
      "420\n",
      "carbon monoxide poisoning\n",
      "662\n",
      "telemarketer protection\n",
      "421\n",
      "industrial waste disposal\n",
      "663\n",
      "Agent Orange exposure\n",
      "301\n",
      "International Organized Crime\n",
      "422\n",
      "art, stolen, forged\n",
      "664\n",
      "American Indian Museum\n",
      "302\n",
      "Poliomyelitis and Post-Polio\n",
      "423\n",
      "Milosevic, Mirjana Markovic\n",
      "665\n",
      "poverty Africa sub-Sahara\n",
      "303\n",
      "Hubble Telescope Achievements\n",
      "424\n",
      "suicides\n",
      "666\n",
      "Thatcher resignation impact\n",
      "304\n",
      "Endangered Species (Mammals)\n",
      "425\n",
      "counterfeiting money\n",
      "667\n",
      "unmarried-partner households\n",
      "305\n",
      "Most Dangerous Vehicles\n",
      "426\n",
      "law enforcement, dogs\n",
      "668\n",
      "poverty, disease\n",
      "306\n",
      "African Civilian Deaths\n",
      "427\n",
      "UV damage, eyes\n",
      "669\n",
      "Islamic Revolution\n",
      "307\n",
      "New Hydroelectric Projects\n",
      "428\n",
      "declining birth rates\n",
      "308\n",
      "Implant Dentistry\n",
      "429\n",
      "Legionnaires' disease\n",
      "309\n",
      "Rap and Crime\n",
      "670\n",
      "U.S. elections apathy\n",
      "671\n",
      "Salvation Army benefits\n",
      "430\n",
      "killer bee attacks\n",
      "672\n",
      "NRA membership profile\n",
      "310\n",
      "Radio Waves and Brain Cancer\n",
      "431\n",
      "robotic technology\n",
      "673\n",
      "Soviet withdrawal Afghanistan\n",
      "311\n",
      "Industrial Espionage\n",
      "432\n",
      "profiling, motorists, police\n",
      "674\n",
      "Greenpeace prosecuted\n",
      "312\n",
      "Hydroponics\n",
      "433\n",
      "Greek, philosophy, stoicism\n",
      "675\n",
      "Olympics training swimming\n",
      "313\n",
      "Magnetic Levitation-Maglev\n",
      "434\n",
      "Estonia, economy\n",
      "676\n",
      "poppy cultivation\n",
      "314\n",
      "Marine Vegetation\n",
      "435\n",
      "curbing population growth\n",
      "677\n",
      "Leaning Tower of Pisa\n",
      "315\n",
      "Unexplained Highway Accidents\n",
      "436\n",
      "railway accidents\n",
      "678\n",
      "joint custody impact\n",
      "316\n",
      "Polygamy Polyandry Polygyny\n",
      "437\n",
      "deregulation, gas, electric\n",
      "679\n",
      "opening adoption records\n",
      "317\n",
      "Unsolicited Faxes\n",
      "438\n",
      "tourism, increase\n",
      "318\n",
      "Best Retirement Country\n",
      "439\n",
      "inventions, scientific discoveries\n",
      "319\n",
      "New Fuel Sources\n",
      "680\n",
      "immigrants Spanish school\n",
      "681\n",
      "wind power location\n",
      "440\n",
      "child labor\n",
      "682\n",
      "adult immigrants English\n",
      "320\n",
      "Undersea Fiber Optic Cable\n",
      "441\n",
      "Lyme disease\n",
      "683\n",
      "Czechoslovakia breakup\n",
      "321\n",
      "Women in Parliaments\n",
      "442\n",
      "heroic acts\n",
      "684\n",
      "part-time benefits\n",
      "322\n",
      "International Art Crime\n",
      "443\n",
      "U.S., investment, Africa\n",
      "685\n",
      "Oscar winner selection\n",
      "323\n",
      "Literary/Journalistic Plagiarism\n",
      "444\n",
      "supercritical fluids\n",
      "686\n",
      "Argentina pegging dollar\n",
      "324\n",
      "Argentine/British Relations\n",
      "445\n",
      "women clergy\n",
      "687\n",
      "Northern Ireland industry\n",
      "325\n",
      "Cult Lifestyles\n",
      "446\n",
      "tourists, violence\n",
      "688\n",
      "non-U.S. media bias\n",
      "326\n",
      "Ferry Sinkings\n",
      "447\n",
      "Stirling engine\n",
      "689\n",
      "family-planning aid\n",
      "327\n",
      "Modern Slavery\n",
      "448\n",
      "ship losses\n",
      "328\n",
      "Pope Beatifications\n",
      "449\n",
      "antibiotics ineffectiveness\n",
      "329\n",
      "Mexican Air Pollution\n",
      "690\n",
      "college education advantage\n",
      "691\n",
      "clear-cutting forests\n",
      "450\n",
      "King Hussein, peace\n",
      "692\n",
      "prostate cancer detection treatment\n",
      "330\n",
      "Iran-Iraq Cooperation\n",
      "693\n",
      "newspapers electronic media\n",
      "331\n",
      "World Bank Criticism\n",
      "694\n",
      "compost pile\n",
      "332\n",
      "Income Tax Evasion\n",
      "695\n",
      "white collar crime sentence\n",
      "333\n",
      "Antibiotics Bacteria Disease\n",
      "696\n",
      "safety plastic surgery\n",
      "334\n",
      "Export Controls Cryptography\n",
      "697\n",
      "air traffic controller\n",
      "335\n",
      "Adoptive Biological Parents\n",
      "698\n",
      "literacy rates Africa\n",
      "336\n",
      "Black Bear Attacks\n",
      "699\n",
      "term limits\n",
      "337\n",
      "Viral Hepatitis\n",
      "338\n",
      "Risk of Aspirin\n",
      "339\n",
      "Alzheimer's Drug Treatment\n",
      "340\n",
      "Land Mine Ban\n",
      "341\n",
      "Airport Security\n",
      "342\n",
      "Diplomatic Expulsion\n",
      "343\n",
      "Police Deaths\n",
      "344\n",
      "Abuses of E-Mail\n",
      "345\n",
      "Overseas Tobacco Sales\n",
      "346\n",
      "Educational Standards\n",
      "347\n",
      "Wildlife Extinction\n",
      "348\n",
      "Agoraphobia\n",
      "349\n",
      "Metabolism\n"
     ]
    }
   ],
   "source": [
    "# Use the topics from the homework to search by document\n",
    "from pyserini.search import get_topics\n",
    "topics = get_topics('robust04')\n",
    "print(len(topics))\n",
    "\n",
    "idx = 0\n",
    "for topic in topics:\n",
    "    if idx < 5:\n",
    "        print(topic)\n",
    "        print(topics[topic]['title'])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8538881190934323, 0.3578883330164455, -0.1872723552896967, -0.3523864200200205, -0.46351744368155007, 0.6468629776968987, 0.6614826316673852, -0.3668020283483838, 0.9602728314242588, 0.3856306968482955]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('embeddings.json') as f:\n",
    "    word_embeddings = json.load(f)\n",
    "print(word_embeddings['octob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = dict()\n",
    "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the embeddings for every word in each document document (you do np.mean() on the embeddings list to do this)\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "VECTOR_SIZE = 10\n",
    "GLOVE_VECTOR_SIZE = 50\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def get_document_embedding(doc_tokens):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens) < 1:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    else:\n",
    "        for token in doc_tokens:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token in word_embeddings:\n",
    "                embeddings.append(word_embeddings[stemmed_token])\n",
    "            else:\n",
    "                embeddings.append(np.random.normal(0, 1, VECTOR_SIZE))\n",
    "        # average the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_glove_document_embedding(doc_tokens):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens) < 1:\n",
    "        return np.zeros(GLOVE_VECTOR_SIZE)\n",
    "    else:\n",
    "        for token in doc_tokens:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token in glove_embeddings:\n",
    "                embeddings.append(glove_embeddings[stemmed_token])\n",
    "            else:\n",
    "                embeddings.append(np.random.normal(0, 1, GLOVE_VECTOR_SIZE))\n",
    "        # average the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "document_embeddings = dict()\n",
    "for doc in tokens_by_doc:\n",
    "    document_embeddings[doc[0]] = get_document_embedding(doc[1])\n",
    "\n",
    "glove_document_embeddings = dict()\n",
    "for doc in tokens_by_doc:\n",
    "    glove_document_embeddings[doc[0]] = get_glove_document_embedding(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When given a query, get its embedding and then do cosine similarity with each of the embedding of the documents\n",
    "import heapq\n",
    "\n",
    "def cosim(v1, v2):\n",
    "  return np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def get_our_top_hundred_documents(query):\n",
    "    # Maybe do something different for query tokens \n",
    "    query_tokens = query.split(' ')\n",
    "    query_embedding = get_document_embedding(query_tokens)\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    idx = 0\n",
    "    for document in document_embeddings:\n",
    "      # Will prob have to change this\n",
    "      document_name = document\n",
    "      document_embedding = document_embeddings[document]\n",
    "      similarity = cosim(document_embedding, query_embedding)\n",
    "      if idx < 100:\n",
    "        heapq.heappush(similarities, (similarity, document_name))\n",
    "      else:\n",
    "        heapq.heapreplace(similarities, (similarity, document_name))\n",
    "      idx += 1\n",
    "    return heapq.nlargest(100, similarities)\n",
    "\n",
    "def get_glove_top_hundred_documents(query):\n",
    "  # Maybe do something different for query tokens \n",
    "    query_tokens = query.split(' ')\n",
    "    query_embedding = get_glove_document_embedding(query_tokens)\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    idx = 0\n",
    "    for document in glove_document_embeddings:\n",
    "      # Will prob have to change this\n",
    "      document_name = document\n",
    "      document_embedding = glove_document_embeddings[document]\n",
    "      similarity = cosim(document_embedding, query_embedding)\n",
    "      if idx < 100:\n",
    "        heapq.heappush(similarities, (similarity, document_name))\n",
    "      else:\n",
    "        heapq.heapreplace(similarities, (similarity, document_name))\n",
    "      idx += 1\n",
    "    return heapq.nlargest(100, similarities)\n",
    "\n",
    "def get_top_hundred_fast_text_documents(query):\n",
    "  hits = searcher.search(query, 100)\n",
    "  return [(hit.score, hit.docid) for hit in hits]\n",
    "\n",
    "\n",
    "def get_top_hundred_ctrl_plus_f_documents(query):\n",
    "  relevant_docs = set()\n",
    "\n",
    "  query_tokens = []\n",
    "  for token in query.split(' '):\n",
    "    query_tokens.append(stemmer.stem(token))\n",
    "  for tup in doc_id_to_tokens:\n",
    "    doc_id = tup[0]\n",
    "    tokens = tup[1]\n",
    "    last_token_index = len(tokens) - 1\n",
    "    current_token_index = 0\n",
    "    occurences = 0\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "      if token == query_tokens[current_token_index]:\n",
    "        if current_token_index == last_token_index:\n",
    "          occurences += 1\n",
    "          current_token_index = 0\n",
    "          \n",
    "        else:\n",
    "          current_token_index += 1\n",
    "    if occurences > 0:\n",
    "      relevant_docs.add((doc_id, occurences))\n",
    "    doc_list = list(relevant_docs)\n",
    "    doc_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return doc_list\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9426818372048587, 'LA100689-0017')\n",
      "(0.9202320420408827, 'LA060389-0042')\n",
      "(0.919560374919724, 'FT932-4890')\n",
      "(0.9142895750234512, 'FBIS3-27512')\n",
      "(0.907285404929815, 'FBIS3-2095')\n",
      "(0.8861437333938965, 'FT923-3828')\n",
      "(0.8838506893854416, 'FT923-4849')\n",
      "(0.8816781025110876, 'FT933-4868')\n",
      "(0.8763627585450621, 'FT944-3280')\n",
      "(0.8749924285238361, 'LA102489-0132')\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# topic is a qid -> dict['title] is the query\n",
    "first_query_ranking = get_our_top_hundred_documents(topics[301]['title'])\n",
    "for i in range(10):\n",
    "    print(first_query_ranking[i])\n",
    "print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_topics = {k:topics[k] for k in list(topics.keys())[:125]}\n",
    "test_topics = {k:topics[k] for k in list(topics.keys())[125:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs(query_id):\n",
    "    relevant_docs = set()\n",
    "    if query_id in qrels:\n",
    "        for document_id in qrels[query_id]:\n",
    "            if document_id in qrels[query_id]:\n",
    "                if qrels[query_id][document_id] == 1:\n",
    "                    relevant_docs.add(document_id)\n",
    "    return relevant_docs\n",
    "\n",
    "# Calculate the MAP@100 for our model, CTRL + F, fasttext, and GloVe\n",
    "def map_hundred(given_topics, model_type):\n",
    "    average_precision_values = []\n",
    "    for topic in given_topics:\n",
    "        query = topics[topic]['title']\n",
    "        actual_relevant_docs = get_relevant_docs(topic)\n",
    "        if model_type == \"ours\":\n",
    "            our_relevant_docs = get_our_top_hundred_documents(query)\n",
    "        elif model_type == \"fasttext\":\n",
    "            our_relevant_docs = get_top_hundred_fast_text_documents(query)\n",
    "        elif model_type == 'ctrl-f':\n",
    "            our_relevant_docs = get_top_hundred_ctrl_plus_f_documents(query)\n",
    "        elif model_type == 'glove':\n",
    "            our_relevant_docs = get_glove_top_hundred_documents(query)\n",
    "        \n",
    "        summed_precision = 0\n",
    "        relevant_count = 0\n",
    "        total_count = 0\n",
    "        for i in range(len(our_relevant_docs)):\n",
    "            should_add = False\n",
    "            if our_relevant_docs[i][1] in actual_relevant_docs:\n",
    "                relevant_count += 1\n",
    "                should_add = True\n",
    "            total_count += 1\n",
    "            if should_add:\n",
    "                summed_precision += (relevant_count / total_count)\n",
    "        if relevant_count > 0:\n",
    "            average_precision_values.append(summed_precision / relevant_count)\n",
    "    average_sum = sum(average_precision_values)\n",
    "    if len(average_precision_values) > 0:\n",
    "        return average_sum / len(average_precision_values)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2361111111111111\n",
      "0.4133287942842544\n",
      "0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "our_map_at_hundred = map_hundred(test_topics, 'ours')\n",
    "print(our_map_at_hundred)\n",
    "fasttext_map_at_hundred = map_hundred(test_topics, 'fasttext')\n",
    "print(fasttext_map_at_hundred)\n",
    "ctrl_f_map_at_hundred = map_hundred(test_topics, 'ctrl-f')\n",
    "print(ctrl_f_map_at_hundred)\n",
    "glove_map_at_100 = map_hundred(test_topics, 'glove')\n",
    "print(glove_map_at_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.24606010343897608, 'FBIS4-20451'), (0.2010871874979118, 'FT934-8936'), (0.19497096592973345, 'FT943-2121')]\n",
      "[(8.458999633789062, 'LA052290-0188'), (7.700099945068359, 'LA060690-0112'), (7.6209001541137695, 'FT922-6787')]\n",
      "[]\n",
      "----------\n",
      "[(0.4478520768234519, 'LA070390-0002'), (0.3396618777163815, 'FT941-2885'), (0.2303453807556575, 'FT934-2731')]\n",
      "[(10.147199630737305, 'FT941-9999'), (9.906599998474121, 'FT934-4848'), (9.766200065612793, 'FT922-15099')]\n",
      "[]\n",
      "----------\n",
      "[(0.6961953241735617, 'LA100689-0099'), (0.626313290366125, 'FT934-16490'), (0.6255856252274765, 'FT934-2731')]\n",
      "[(8.918700218200684, 'FT934-11803'), (8.74779987335205, 'LA120290-0163'), (8.687600135803223, 'FT934-10925')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# relevant docs for 350, 351, 352\n",
    "query_one = topics[350]['title']\n",
    "query_two = topics[351]['title']\n",
    "query_three = topics[352]['title']\n",
    "\n",
    "our_top_3_one = get_our_top_hundred_documents(query_one)[:3]\n",
    "fast_text_top_3_one = get_top_hundred_fast_text_documents(query_one)[:3]\n",
    "ctrl_f_top_3_one = get_top_hundred_ctrl_plus_f_documents(query_one)[:3]\n",
    "glove_top_3_one = get_glove_top_hundred_documents(query_one)[:3]\n",
    "\n",
    "our_top_3_two = get_our_top_hundred_documents(query_two)[:3]\n",
    "fast_text_top_3_two = get_top_hundred_fast_text_documents(query_two)[:3]\n",
    "ctrl_f_top_3_two = get_top_hundred_ctrl_plus_f_documents(query_two)[:3]\n",
    "glove_top_3_two = get_glove_top_hundred_documents(query_two)[:3]\n",
    "\n",
    "our_top_3_three = get_our_top_hundred_documents(query_three)[:3]\n",
    "fast_text_top_3_three = get_top_hundred_fast_text_documents(query_three)[:3]\n",
    "ctrl_f_top_3_three = get_top_hundred_ctrl_plus_f_documents(query_three)[:3]\n",
    "glove_top_3_three = get_glove_top_hundred_documents(query_three)[:3]\n",
    "\n",
    "print(our_top_3_one)\n",
    "print(fast_text_top_3_one)\n",
    "print(ctrl_f_top_3_one)\n",
    "print(glove_top_3_one)\n",
    "print(\"----------\")\n",
    "print(our_top_3_two)\n",
    "print(fast_text_top_3_two)\n",
    "print(ctrl_f_top_3_two)\n",
    "print(glove_top_3_two)\n",
    "print(\"----------\")\n",
    "print(our_top_3_three)\n",
    "print(fast_text_top_3_three)\n",
    "print(ctrl_f_top_3_three)\n",
    "print(glove_top_3_three)\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[350]:\n",
    "#     if qrels[350][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[351]:\n",
    "#     if qrels[351][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[352]:\n",
    "#     if qrels[352][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
