{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserini==0.12.0 in /opt/homebrew/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (4.64.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (0.1.97)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.23.5)\n",
      "Requirement already satisfied: Cython>=0.29.21 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (0.29.32)\n",
      "Requirement already satisfied: pyjnius>=1.2.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (1.9.3)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/homebrew/lib/python3.10/site-packages (from pyserini==0.12.0) (4.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from pandas>=1.1.5->pyserini==0.12.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.1.5->pyserini==0.12.0) (2022.6)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from pyjnius>=1.2.1->pyserini==0.12.0) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=0.22.1->pyserini==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=0.22.1->pyserini==0.12.0) (3.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (0.11.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (0.13.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (3.8.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.0.0->pyserini==0.12.0) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.0.0->pyserini==0.12.0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hstack/Library/Python/3.10/lib/python/site-packages (from packaging>=20.0->transformers>=4.0.0->pyserini==0.12.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.0.0->pyserini==0.12.0) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement glove-python-binary (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for glove-python-binary\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting glove\n",
      "  Using cached glove-1.0.2.tar.gz (44 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from glove) (1.23.5)\n",
      "Building wheels for collected packages: glove\n",
      "  Building wheel for glove (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/__init__.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/glove.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing glove.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to glove.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to glove.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: Error: setup script specifies an absolute path:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     /private/var/folders/pf/yhbc_83j0_16nz__1n8lp9qr0000gn/T/pip-install-8k3kxtoj/glove_44d1b599f9c64322a980417a0a7c4400/glove/glove_inner.pyx\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m setup() arguments must *always* be /-separated paths relative to the\n",
      "  \u001b[31m   \u001b[0m setup.py directory, *never* absolute paths.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for glove\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for glove\n",
      "Failed to build glove\n",
      "Installing collected packages: glove\n",
      "  Running setup.py install for glove ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for glove\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[25 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /opt/homebrew/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/__init__.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m copying glove/glove.py -> build/lib.macosx-12-arm64-cpython-310/glove\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing glove.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to glove.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to glove.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: Error: setup script specifies an absolute path:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     /private/var/folders/pf/yhbc_83j0_16nz__1n8lp9qr0000gn/T/pip-install-8k3kxtoj/glove_44d1b599f9c64322a980417a0a7c4400/glove/glove_inner.pyx\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m setup() arguments must *always* be /-separated paths relative to the\n",
      "  \u001b[31m   \u001b[0m setup.py directory, *never* absolute paths.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m glove\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hstack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install pyserini==0.12.0\n",
    "%pip install glove-python-binary\n",
    "%pip install glove\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "from pyserini import search\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174787 FBIS4-32573\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "qfile = 'https://github.com/castorini/anserini-tools/blob/63ceeab1dd94c1221f29b931d868e8fab67cc25c/topics-and-qrels/qrels.robust04.txt?raw=true'\n",
    "docids = set()\n",
    "# queryId -> docId -> relevanceScore\n",
    "qrels = dict()\n",
    "for line in urlopen(qfile):\n",
    "  qid, round, docid, score = line.strip().split()\n",
    "  new_docid = docid.decode('UTF-8')\n",
    "  docids.add(new_docid)\n",
    "\n",
    "  if int(qid) in qrels:\n",
    "    qrels[int(qid)][new_docid] = int(score)\n",
    "  else:\n",
    "    qrels[int(qid)] = dict()\n",
    "    qrels[int(qid)][new_docid] = int(score)\n",
    "\n",
    "\n",
    "docids = list(docids)\n",
    "np.random.shuffle(docids)\n",
    "print(len(docids), docids[0])\n",
    "print(len(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index robust04.\n",
      "/Users/hstack/.cache/pyserini/indexes/index-robust04-20191213.15f3d001489c97849a010b0a4734d018 already exists, skipping download.\n",
      "Initializing robust04...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('FBIS4-32573',\n",
       " '  BFN  <F P=106> [Report by editor Tomas Sulak on the second regular congress </F> of the Christian Social Union in Moravia on 25 June recorded] [Text] [Sulak] The Christian Social Union [KSU], formed by the opposition to the leadership of the Christian Democratic Union-Czechoslovakia Peoples Party [KDU-CSL], headed by Josef Lux, held a congress today. The participants are in favor of following the spirit of the prewar Czechoslovak Peoples Party headed by Jan Sramek. Milos Hanak, former member of the Peoples Party and the current chairman of the Vyskov, Moravia District privatization commission, was elected the new chairman of the party. Four new deputy chairmen and a new central committee of the party were also elected. The congress stripped former party Chairman Frantisek Michalek of all party posts. According to its new chairman, Hanak, the KSU is currently operating within the framework of the so-called Realistic Bloc, considers itself to be a party of the center, and does not consider itself to be a regional party. On the contrary, it will attempt to address citizens of the whole Czech Republic. The party is based on the principle of Christian humanism, which is being perceived as an alternative to both dogmatic collectivism and unrestrained liberalism. It also emphasizes the social aspect of political life, and its political priority is to renew land-based administration [zemske zrizeni] in Moravia.   ')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "searcher = search.SimpleSearcher.from_prebuilt_index('robust04')\n",
    "for docid in docids[:50]:\n",
    "  try:\n",
    "    text = searcher.doc(docid).raw()\n",
    "    data.append((docid, text))\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "cleaned_data = []\n",
    "for docid, text in data:\n",
    "  new_text = text.replace('<P>', ' ')\n",
    "  new_text = new_text.replace('</P>', ' ')\n",
    "  new_text = new_text.replace('<DATE>', ' ')\n",
    "  new_text = new_text.replace('</DATE>', ' ')\n",
    "  new_text = new_text.replace('<HEADLINE>', ' ')\n",
    "  new_text = new_text.replace('</HEADLINE>', ' ')\n",
    "  new_text = new_text.replace('<TEXT>', ' ')\n",
    "  new_text = new_text.replace('</TEXT>', ' ')\n",
    "  new_text = new_text.replace('\\n', ' ')\n",
    "  new_text = new_text.replace(\"\\'\", '')\n",
    "  new_text = new_text.replace('--', ' ')\n",
    "  cleaned_data.append((docid, new_text))\n",
    "  \n",
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FBIS4-32573', ['bfn', 'f', 'p', 'report', 'editor', 'toma', 'sulak', 'second', 'regular', 'congress', 'f', 'christian', 'social', 'union', 'moravia', 'june', 'record', 'text', 'sulak', 'christian', 'social', 'union', 'ksu', 'form', 'opposit', 'leadership', 'christian', 'democrat', 'union', 'czechoslovakia', 'peopl', 'parti', 'kdu', 'csl', 'head', 'josef', 'lux', 'held', 'congress', 'today', 'particip', 'favor', 'follow', 'spirit', 'prewar', 'czechoslovak', 'peopl', 'parti', 'head', 'jan', 'sramek', 'milo', 'hanak', 'former', 'member', 'peopl', 'parti', 'current', 'chairman', 'vyskov', 'moravia', 'district', 'privat', 'commiss', 'wa', 'elect', 'new', 'chairman', 'parti', 'four', 'new', 'deputi', 'chairmen', 'new', 'central', 'committe', 'parti', 'also', 'elect', 'congress', 'strip', 'former', 'parti', 'chairman', 'frantisek', 'michalek', 'parti', 'post', 'accord', 'new', 'chairman', 'hanak', 'ksu', 'current', 'oper', 'within', 'framework', 'call', 'realist', 'bloc', 'consid', 'parti', 'center', 'doe', 'consid', 'region', 'parti', 'contrari', 'attempt', 'address', 'citizen', 'whole', 'czech', 'republ', 'parti', 'base', 'principl', 'christian', 'human', 'perceiv', 'altern', 'dogmat', 'collectiv', 'unrestrain', 'liber', 'also', 'emphas', 'social', 'aspect', 'polit', 'life', 'polit', 'prioriti', 'renew', 'land', 'base', 'administr', 'zemsk', 'zrizeni', 'moravia'])\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def tokenize(doc_id_and_text):\n",
    "    text = doc_id_and_text[1]\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    tokens = pattern.findall(text)\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "      stemmed_token = stemmer.stem(token.replace(\"'\", ''))\n",
    "      if stemmed_token not in stop_words:\n",
    "        stemmed_tokens.append(stemmed_token)\n",
    "    return (doc_id_and_text[0],stemmed_tokens)\n",
    "\n",
    "tokens_by_doc = [tokenize(doc) for doc in cleaned_data[:25]]\n",
    "print(tokens_by_doc[0])\n",
    "\n",
    "doc_id_to_tokens = dict()\n",
    "for tokens in tokens_by_doc:\n",
    "  doc_id_to_tokens[tokens[0]] = tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "350\n",
      "Health and Computer Terminals\n",
      "351\n",
      "Falkland petroleum exploration\n",
      "352\n",
      "British Chunnel impact\n",
      "353\n",
      "Antarctica exploration\n",
      "354\n",
      "journalist risks\n",
      "355\n",
      "ocean remote sensing\n",
      "356\n",
      "postmenopausal estrogen Britain\n",
      "357\n",
      "territorial waters dispute\n",
      "358\n",
      "blood-alcohol fatalities\n",
      "359\n",
      "mutual fund predictors\n",
      "360\n",
      "drug legalization benefits\n",
      "361\n",
      "clothing sweatshops\n",
      "362\n",
      "human smuggling\n",
      "363\n",
      "transportation tunnel disasters\n",
      "364\n",
      "rabies\n",
      "365\n",
      "El Nino\n",
      "366\n",
      "commercial cyanide uses\n",
      "367\n",
      "piracy\n",
      "368\n",
      "in vitro fertilization\n",
      "369\n",
      "anorexia nervosa bulimia\n",
      "370\n",
      "food/drug laws\n",
      "371\n",
      "health insurance holistic\n",
      "372\n",
      "Native American casino\n",
      "373\n",
      "encryption equipment export\n",
      "374\n",
      "Nobel prize winners\n",
      "375\n",
      "hydrogen energy\n",
      "376\n",
      "World Court\n",
      "377\n",
      "cigar smoking\n",
      "378\n",
      "euro opposition\n",
      "379\n",
      "mainstreaming\n",
      "380\n",
      "obesity medical treatment\n",
      "381\n",
      "alternative medicine\n",
      "382\n",
      "hydrogen fuel automobiles\n",
      "383\n",
      "mental illness drugs\n",
      "384\n",
      "space station moon\n",
      "385\n",
      "hybrid fuel cars\n",
      "386\n",
      "teaching disabled children\n",
      "387\n",
      "radioactive waste\n",
      "388\n",
      "organic soil enhancement\n",
      "389\n",
      "illegal technology transfer\n",
      "700\n",
      "gasoline tax U.S.\n",
      "390\n",
      "orphan drugs\n",
      "391\n",
      "R&D drug prices\n",
      "392\n",
      "robotics\n",
      "393\n",
      "mercy killing\n",
      "394\n",
      "home schooling\n",
      "395\n",
      "tourism\n",
      "396\n",
      "sick building syndrome\n",
      "397\n",
      "automobile recalls\n",
      "398\n",
      "dismantling Europe's arsenal\n",
      "399\n",
      "oceanographic vessels\n",
      "601\n",
      "Turkey Iraq water\n",
      "602\n",
      "Czech, Slovak sovereignty\n",
      "603\n",
      "Tobacco cigarette lawsuit\n",
      "604\n",
      "Lyme disease arthritis\n",
      "605\n",
      "Great Britain health care\n",
      "606\n",
      "leg traps ban\n",
      "607\n",
      "human genetic code\n",
      "608\n",
      "taxing social security\n",
      "609\n",
      "per capita alcohol consumption\n",
      "610\n",
      "minimum wage adverse impact\n",
      "611\n",
      "Kurds Germany violence\n",
      "612\n",
      "Tibet protesters\n",
      "613\n",
      "Berlin wall disposal\n",
      "614\n",
      "Flavr Savr tomato\n",
      "615\n",
      "timber exports Asia\n",
      "616\n",
      "Volkswagen Mexico\n",
      "617\n",
      "Russia Cuba economy\n",
      "618\n",
      "Ayatollah Khomeini death\n",
      "619\n",
      "Winnie Mandela scandal\n",
      "620\n",
      "France nuclear testing\n",
      "621\n",
      "women ordained Church of England\n",
      "622\n",
      "price fixing\n",
      "623\n",
      "toxic chemical weapon\n",
      "624\n",
      "SDI Star Wars\n",
      "625\n",
      "arrests bombing WTC\n",
      "626\n",
      "human stampede\n",
      "627\n",
      "Russian food crisis\n",
      "628\n",
      "U.S. invasion of Panama\n",
      "629\n",
      "abortion clinic attack\n",
      "630\n",
      "Gulf War Syndrome\n",
      "631\n",
      "Mandela South Africa President\n",
      "632\n",
      "southeast Asia tin mining\n",
      "633\n",
      "Welsh devolution\n",
      "634\n",
      "L-tryptophan deaths\n",
      "635\n",
      "doctor assisted suicides\n",
      "636\n",
      "jury duty exemptions\n",
      "637\n",
      "human growth hormone (HGH)\n",
      "638\n",
      "wrongful convictions\n",
      "639\n",
      "consumer on-line shopping\n",
      "640\n",
      "maternity leave policies\n",
      "641\n",
      "Valdez wildlife marine life\n",
      "400\n",
      "Amazon rain forest\n",
      "642\n",
      "Tiananmen Square protesters\n",
      "401\n",
      "foreign minorities, Germany\n",
      "643\n",
      "salmon dams Pacific northwest\n",
      "402\n",
      "behavioral genetics\n",
      "644\n",
      "exotic animals import\n",
      "403\n",
      "osteoporosis\n",
      "645\n",
      "software piracy\n",
      "404\n",
      "Ireland, peace talks\n",
      "646\n",
      "food stamps increase\n",
      "405\n",
      "cosmic events\n",
      "647\n",
      "windmill electricity\n",
      "406\n",
      "Parkinson's disease\n",
      "648\n",
      "family leave law\n",
      "407\n",
      "poaching, wildlife preserves\n",
      "649\n",
      "computer viruses\n",
      "408\n",
      "tropical storms\n",
      "409\n",
      "legal, Pan Am, 103\n",
      "650\n",
      "tax evasion indicted\n",
      "651\n",
      "U.S. ethnic population\n",
      "410\n",
      "Schengen agreement\n",
      "652\n",
      "OIC Balkans 1990s\n",
      "411\n",
      "salvaging, shipwreck, treasure\n",
      "653\n",
      "ETA Basque terrorism\n",
      "412\n",
      "airport security\n",
      "654\n",
      "same-sex schools\n",
      "413\n",
      "steel production\n",
      "655\n",
      "ADD diagnosis treatment\n",
      "414\n",
      "Cuba, sugar, exports\n",
      "656\n",
      "lead poisoning children\n",
      "415\n",
      "drugs, Golden Triangle\n",
      "657\n",
      "school prayer banned\n",
      "416\n",
      "Three Gorges Project\n",
      "658\n",
      "teenage pregnancy\n",
      "417\n",
      "creativity\n",
      "659\n",
      "cruise health safety\n",
      "418\n",
      "quilts, income\n",
      "419\n",
      "recycle, automobile tires\n",
      "660\n",
      "whale watching California\n",
      "661\n",
      "melanoma treatment causes\n",
      "420\n",
      "carbon monoxide poisoning\n",
      "662\n",
      "telemarketer protection\n",
      "421\n",
      "industrial waste disposal\n",
      "663\n",
      "Agent Orange exposure\n",
      "301\n",
      "International Organized Crime\n",
      "422\n",
      "art, stolen, forged\n",
      "664\n",
      "American Indian Museum\n",
      "302\n",
      "Poliomyelitis and Post-Polio\n",
      "423\n",
      "Milosevic, Mirjana Markovic\n",
      "665\n",
      "poverty Africa sub-Sahara\n",
      "303\n",
      "Hubble Telescope Achievements\n",
      "424\n",
      "suicides\n",
      "666\n",
      "Thatcher resignation impact\n",
      "304\n",
      "Endangered Species (Mammals)\n",
      "425\n",
      "counterfeiting money\n",
      "667\n",
      "unmarried-partner households\n",
      "305\n",
      "Most Dangerous Vehicles\n",
      "426\n",
      "law enforcement, dogs\n",
      "668\n",
      "poverty, disease\n",
      "306\n",
      "African Civilian Deaths\n",
      "427\n",
      "UV damage, eyes\n",
      "669\n",
      "Islamic Revolution\n",
      "307\n",
      "New Hydroelectric Projects\n",
      "428\n",
      "declining birth rates\n",
      "308\n",
      "Implant Dentistry\n",
      "429\n",
      "Legionnaires' disease\n",
      "309\n",
      "Rap and Crime\n",
      "670\n",
      "U.S. elections apathy\n",
      "671\n",
      "Salvation Army benefits\n",
      "430\n",
      "killer bee attacks\n",
      "672\n",
      "NRA membership profile\n",
      "310\n",
      "Radio Waves and Brain Cancer\n",
      "431\n",
      "robotic technology\n",
      "673\n",
      "Soviet withdrawal Afghanistan\n",
      "311\n",
      "Industrial Espionage\n",
      "432\n",
      "profiling, motorists, police\n",
      "674\n",
      "Greenpeace prosecuted\n",
      "312\n",
      "Hydroponics\n",
      "433\n",
      "Greek, philosophy, stoicism\n",
      "675\n",
      "Olympics training swimming\n",
      "313\n",
      "Magnetic Levitation-Maglev\n",
      "434\n",
      "Estonia, economy\n",
      "676\n",
      "poppy cultivation\n",
      "314\n",
      "Marine Vegetation\n",
      "435\n",
      "curbing population growth\n",
      "677\n",
      "Leaning Tower of Pisa\n",
      "315\n",
      "Unexplained Highway Accidents\n",
      "436\n",
      "railway accidents\n",
      "678\n",
      "joint custody impact\n",
      "316\n",
      "Polygamy Polyandry Polygyny\n",
      "437\n",
      "deregulation, gas, electric\n",
      "679\n",
      "opening adoption records\n",
      "317\n",
      "Unsolicited Faxes\n",
      "438\n",
      "tourism, increase\n",
      "318\n",
      "Best Retirement Country\n",
      "439\n",
      "inventions, scientific discoveries\n",
      "319\n",
      "New Fuel Sources\n",
      "680\n",
      "immigrants Spanish school\n",
      "681\n",
      "wind power location\n",
      "440\n",
      "child labor\n",
      "682\n",
      "adult immigrants English\n",
      "320\n",
      "Undersea Fiber Optic Cable\n",
      "441\n",
      "Lyme disease\n",
      "683\n",
      "Czechoslovakia breakup\n",
      "321\n",
      "Women in Parliaments\n",
      "442\n",
      "heroic acts\n",
      "684\n",
      "part-time benefits\n",
      "322\n",
      "International Art Crime\n",
      "443\n",
      "U.S., investment, Africa\n",
      "685\n",
      "Oscar winner selection\n",
      "323\n",
      "Literary/Journalistic Plagiarism\n",
      "444\n",
      "supercritical fluids\n",
      "686\n",
      "Argentina pegging dollar\n",
      "324\n",
      "Argentine/British Relations\n",
      "445\n",
      "women clergy\n",
      "687\n",
      "Northern Ireland industry\n",
      "325\n",
      "Cult Lifestyles\n",
      "446\n",
      "tourists, violence\n",
      "688\n",
      "non-U.S. media bias\n",
      "326\n",
      "Ferry Sinkings\n",
      "447\n",
      "Stirling engine\n",
      "689\n",
      "family-planning aid\n",
      "327\n",
      "Modern Slavery\n",
      "448\n",
      "ship losses\n",
      "328\n",
      "Pope Beatifications\n",
      "449\n",
      "antibiotics ineffectiveness\n",
      "329\n",
      "Mexican Air Pollution\n",
      "690\n",
      "college education advantage\n",
      "691\n",
      "clear-cutting forests\n",
      "450\n",
      "King Hussein, peace\n",
      "692\n",
      "prostate cancer detection treatment\n",
      "330\n",
      "Iran-Iraq Cooperation\n",
      "693\n",
      "newspapers electronic media\n",
      "331\n",
      "World Bank Criticism\n",
      "694\n",
      "compost pile\n",
      "332\n",
      "Income Tax Evasion\n",
      "695\n",
      "white collar crime sentence\n",
      "333\n",
      "Antibiotics Bacteria Disease\n",
      "696\n",
      "safety plastic surgery\n",
      "334\n",
      "Export Controls Cryptography\n",
      "697\n",
      "air traffic controller\n",
      "335\n",
      "Adoptive Biological Parents\n",
      "698\n",
      "literacy rates Africa\n",
      "336\n",
      "Black Bear Attacks\n",
      "699\n",
      "term limits\n",
      "337\n",
      "Viral Hepatitis\n",
      "338\n",
      "Risk of Aspirin\n",
      "339\n",
      "Alzheimer's Drug Treatment\n",
      "340\n",
      "Land Mine Ban\n",
      "341\n",
      "Airport Security\n",
      "342\n",
      "Diplomatic Expulsion\n",
      "343\n",
      "Police Deaths\n",
      "344\n",
      "Abuses of E-Mail\n",
      "345\n",
      "Overseas Tobacco Sales\n",
      "346\n",
      "Educational Standards\n",
      "347\n",
      "Wildlife Extinction\n",
      "348\n",
      "Agoraphobia\n",
      "349\n",
      "Metabolism\n"
     ]
    }
   ],
   "source": [
    "# Use the topics from the homework to search by document\n",
    "from pyserini.search import get_topics\n",
    "topics = get_topics('robust04')\n",
    "print(len(topics))\n",
    "\n",
    "idx = 0\n",
    "for topic in topics:\n",
    "    if idx < 5:\n",
    "        print(topic)\n",
    "        print(topics[topic]['title'])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8538881190934323, 0.3578883330164455, -0.1872723552896967, -0.3523864200200205, -0.46351744368155007, 0.6468629776968987, 0.6614826316673852, -0.3668020283483838, 0.9602728314242588, 0.3856306968482955]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('embeddings.json') as f:\n",
    "    word_embeddings = json.load(f)\n",
    "print(word_embeddings['octob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the embeddings for every word in each document document (you do np.mean() on the embeddings list to do this)\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "VECTOR_SIZE = 10\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def get_document_embedding(doc_tokens):\n",
    "    embeddings = []\n",
    "    if len(doc_tokens) < 1:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    else:\n",
    "        for token in doc_tokens:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            if stemmed_token in word_embeddings:\n",
    "                embeddings.append(word_embeddings[stemmed_token])\n",
    "            else:\n",
    "                embeddings.append(np.random.normal(0, 1, VECTOR_SIZE))\n",
    "        # average the vectors of individual words to get the vector of the document\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "document_embeddings = dict()\n",
    "for doc in tokens_by_doc:\n",
    "    document_embeddings[doc[0]] = get_document_embedding(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When given a query, get its embedding and then do cosine similarity with each of the embedding of the documents\n",
    "import heapq\n",
    "\n",
    "def cosim(v1, v2):\n",
    "  return np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def get_our_top_hundred_documents(query):\n",
    "    # Maybe do something different for query tokens \n",
    "    query_tokens = query.split(' ')\n",
    "    query_embedding = get_document_embedding(query_tokens)\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    idx = 0\n",
    "    for document in document_embeddings:\n",
    "      # Will prob have to change this\n",
    "      document_name = document\n",
    "      document_embedding = document_embeddings[document]\n",
    "      similarity = cosim(document_embedding, query_embedding)\n",
    "      if idx < 100:\n",
    "        heapq.heappush(similarities, (similarity, document_name))\n",
    "      else:\n",
    "        heapq.heapreplace(similarities, (similarity, document_name))\n",
    "      idx += 1\n",
    "    return heapq.nlargest(100, similarities)\n",
    "\n",
    "def get_top_hundred_fast_text_documents(query):\n",
    "  hits = searcher.search(query, 100)\n",
    "  return [(hit.score, hit.docid) for hit in hits]\n",
    "\n",
    "\n",
    "def get_top_hundred_ctrl_plus_f_documents(query):\n",
    "  relevant_docs = set()\n",
    "\n",
    "  query_tokens = []\n",
    "  for token in query.split(' '):\n",
    "    query_tokens.append(stemmer.stem(token))\n",
    "  for tup in doc_id_to_tokens:\n",
    "    doc_id = tup[0]\n",
    "    tokens = tup[1]\n",
    "    # last_token_index = len(tokens) - 1\n",
    "    # current_token_index = 0\n",
    "    occurences = 0\n",
    "\n",
    "    for token in tokens:\n",
    "      if stemmer.stem(token) in query_tokens:\n",
    "        occurences += 1\n",
    "      # if token == query_tokens[current_token_index]:\n",
    "      #   if current_token_index == last_token_index:\n",
    "      #     occurences += 1\n",
    "      #     current_token_index = 0\n",
    "          \n",
    "      #   else:\n",
    "      #     current_token_index += 1\n",
    "    if occurences > 0:\n",
    "      relevant_docs.add((doc_id, occurences))\n",
    "    doc_list = list(relevant_docs)\n",
    "    doc_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return doc_list\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9631266454888019, 'FBIS4-18004')\n",
      "(0.9228843446963603, 'LA052090-0019')\n",
      "(0.9202195824039938, 'FBIS3-1476')\n",
      "(0.9133623374711642, 'FBIS4-50071')\n",
      "(0.9017937317931818, 'FBIS4-43047')\n",
      "(0.9000857829795657, 'FT934-16490')\n",
      "(0.8979535928831708, 'FT943-1007')\n",
      "(0.8895116017459416, 'FBIS3-59833')\n",
      "(0.8843022866862087, 'FT934-2731')\n",
      "(0.8815653978588323, 'FT942-13766')\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# topic is a qid -> dict['title] is the query\n",
    "first_query_ranking = get_our_top_hundred_documents(topics[301]['title'])\n",
    "for i in range(10):\n",
    "    print(first_query_ranking[i])\n",
    "print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_topics = {k:topics[k] for k in list(topics.keys())[:125]}\n",
    "test_topics = {k:topics[k] for k in list(topics.keys())[125:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs(query_id):\n",
    "    relevant_docs = set()\n",
    "    if query_id in qrels:\n",
    "        for document_id in qrels[query_id]:\n",
    "            if document_id in qrels[query_id]:\n",
    "                if qrels[query_id][document_id] == 1:\n",
    "                    relevant_docs.add(document_id)\n",
    "    return relevant_docs\n",
    "\n",
    "# Calculate the MAP@100 for our model, CTRL + F, fasttext, and GloVe\n",
    "def map_hundred(given_topics, model_type):\n",
    "    average_precision_values = []\n",
    "    for topic in given_topics:\n",
    "        query = topics[topic]['title']\n",
    "        actual_relevant_docs = get_relevant_docs(topic)\n",
    "        if model_type == \"ours\":\n",
    "            our_relevant_docs = get_our_top_hundred_documents(query)\n",
    "        elif model_type == \"fasttext\":\n",
    "            our_relevant_docs = get_top_hundred_fast_text_documents(query)\n",
    "        elif model_type == 'ctrl-f':\n",
    "            our_relevant_docs = get_top_hundred_ctrl_plus_f_documents(query)\n",
    "        \n",
    "        summed_precision = 0\n",
    "        relevant_count = 0\n",
    "        total_count = 0\n",
    "        for i in range(len(our_relevant_docs)):\n",
    "            should_add = False\n",
    "            if our_relevant_docs[i][1] in actual_relevant_docs:\n",
    "                relevant_count += 1\n",
    "                should_add = True\n",
    "            total_count += 1\n",
    "            if should_add:\n",
    "                summed_precision += (relevant_count / total_count)\n",
    "        if relevant_count > 0:\n",
    "            average_precision_values.append(summed_precision / relevant_count)\n",
    "    average_sum = sum(average_precision_values)\n",
    "    if len(average_precision_values) > 0:\n",
    "        return average_sum / len(average_precision_values)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.4133287942842544\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "our_map_at_hundred = map_hundred(test_topics, 'ours')\n",
    "print(our_map_at_hundred)\n",
    "fasttext_map_at_hundred = map_hundred(test_topics, 'fasttext')\n",
    "print(fasttext_map_at_hundred)\n",
    "ctrl_f_map_at_hundred = map_hundred(test_topics, 'ctrl-f')\n",
    "print(ctrl_f_map_at_hundred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.24606010343897608, 'FBIS4-20451'), (0.2010871874979118, 'FT934-8936'), (0.19497096592973345, 'FT943-2121')]\n",
      "[(8.458999633789062, 'LA052290-0188'), (7.700099945068359, 'LA060690-0112'), (7.6209001541137695, 'FT922-6787')]\n",
      "[]\n",
      "----------\n",
      "[(0.4478520768234519, 'LA070390-0002'), (0.3396618777163815, 'FT941-2885'), (0.2303453807556575, 'FT934-2731')]\n",
      "[(10.147199630737305, 'FT941-9999'), (9.906599998474121, 'FT934-4848'), (9.766200065612793, 'FT922-15099')]\n",
      "[]\n",
      "----------\n",
      "[(0.6961953241735617, 'LA100689-0099'), (0.626313290366125, 'FT934-16490'), (0.6255856252274765, 'FT934-2731')]\n",
      "[(8.918700218200684, 'FT934-11803'), (8.74779987335205, 'LA120290-0163'), (8.687600135803223, 'FT934-10925')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# relevant docs for 350, 351, 352\n",
    "query_one = topics[350]['title']\n",
    "query_two = topics[351]['title']\n",
    "query_three = topics[352]['title']\n",
    "\n",
    "our_top_3_one = get_our_top_hundred_documents(query_one)[:3]\n",
    "fast_text_top_3_one = get_top_hundred_fast_text_documents(query_one)[:3]\n",
    "ctrl_f_top_3_one = get_top_hundred_ctrl_plus_f_documents(query_one)[:3]\n",
    "\n",
    "our_top_3_two = get_our_top_hundred_documents(query_two)[:3]\n",
    "fast_text_top_3_two = get_top_hundred_fast_text_documents(query_two)[:3]\n",
    "ctrl_f_top_3_two = get_top_hundred_ctrl_plus_f_documents(query_two)[:3]\n",
    "\n",
    "our_top_3_three = get_our_top_hundred_documents(query_three)[:3]\n",
    "fast_text_top_3_three = get_top_hundred_fast_text_documents(query_three)[:3]\n",
    "ctrl_f_top_3_three = get_top_hundred_ctrl_plus_f_documents(query_three)[:3]\n",
    "\n",
    "print(our_top_3_one)\n",
    "print(fast_text_top_3_one)\n",
    "print(ctrl_f_top_3_one)\n",
    "print(\"----------\")\n",
    "print(our_top_3_two)\n",
    "print(fast_text_top_3_two)\n",
    "print(ctrl_f_top_3_two)\n",
    "print(\"----------\")\n",
    "print(our_top_3_three)\n",
    "print(fast_text_top_3_three)\n",
    "print(ctrl_f_top_3_three)\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[350]:\n",
    "#     if qrels[350][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[351]:\n",
    "#     if qrels[351][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")\n",
    "# relevant_docs = []\n",
    "# for doc_id in qrels[352]:\n",
    "#     if qrels[352][doc_id] == 1:\n",
    "#         relevant_docs.append(doc_id)\n",
    "# print(relevant_docs)\n",
    "# print(\"---------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
